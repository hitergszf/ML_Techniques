# Radial Basis Function Network

### 一、RBF Network Hypothesis

1. 回顾一下kernel SVM：我们在无限维度的空间中找到large margin然后用$\alpha_n$来结合这些以$x_n$为中心的高斯函数

   ![image-20200220175517522](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220175517522.png)

高斯核是径向基的一种，所谓径向基包含两个部分：

- radial: 径向就是半径，其实就是一个欧氏距离的度量
- basis function：就是combine这些东西在一起，可以想象成是一种aggregation model

**RBF Network: linear aggregation of radial hypothesis**

2. 对比一下NN 和 RBF Network:![image-20200220180007931](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220180007931.png)

   hidden layer不一样，神经网络是tanh作non-linear的内积层，RBF网络是用这些输入作为center的aggregation vote

3. 我们设计一下RBF的一般结构如下:

   投票的权重：$\beta_m$，RBF center：$\mu_m$

   ![image-20200220180248599](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220180248599.png)

回顾一下我们在SVM中做的事情，我们是直接把$x_1,x_2,...$当作了center，然后把他们的输出$y_1,y_2$作为了votes。一般的RBF网络不同，我们需要学习center 和 votes：

我们首先理解一下RBF Network做的事情，其实就是学一些距离之间的相似关系，就类似于kernel Z域内积的相似性，RBF是X域和中心的相似性（距离度量）他们都受限于Mercer's Condition



### 二、RBF Network Learning

1. 看一下Full RBF Network model它就是把所有的数据都拿来当作中心。其实这个是一个偷懒的方式而且有的时候效果并不是特别好。![image-20200220181316363](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220181316363.png)

它其实类似于K-nearest neighbor算法：selection而不是aggregate，因为高斯函数的波动比较大，往往最近的那个就dominate所有的结果了。这个时候aggregate就变成了selection。

![image-20200220181343161](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220181343161.png)

2. Full RBF Network里面的插值操作：我们需要对这个模型作MSE的最小化：
   $$
   h{(x)}=\Big(\sum_{m=1}^N\beta_mRBF(x,x_m)\Big)
   $$
   首先我们定义一个matrix：
   $$
   z_n = [RBF(x_n,x_1),RBF(x_n,x_2),...,RBF(x_n,x_N)]
   $$
   类似于一个transformation，那么根据之前学过的linear regression的知识，我们直到如果$Z^TZ$是可逆的，那么就有
   $$
   \beta = (Z^TZ)^{-1}Zy
   $$
   下面我们证明$x_n$不同的情况下Z可逆：

   因为Gaussian Kernel Matrix是对称的，所以说行列对应相等。

   于是我们可以继续化简：
   $$
   \beta = Z^{-1}y
   $$
   **So Easy!**

   ![image-20200220184758878](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220184758878.png)

3. Regularized Full RBF Network

   我们看一下之前的结果：

   ![image-20200220185056986](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220185056986.png)

   ![image-20200220185113670](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220185113670.png)

   这个完美的插值做的太好了！这样似乎很容易过拟合。。。为了防止过拟合，我们加入正则项，类比之前的脊回归：
   $$
   \beta = \Big(Z^TZ+\lambda I\Big)^{-1}Z^Ty
   $$
   我们注意到其实Z就是高斯核矩阵，对比一下各种空间里面的正则化机制：

   ![image-20200220185354384](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220185354384.png)

4. 我们还可以通过减少中心来减少模型复杂度，类似于SVM中只考虑SV的作用：![image-20200220185522555](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220185522555.png)

这里我们就需要考虑如何减少这些centers（原型）接下来的算法就可以解决这个问题。

### 三、K-Means Algorithm

1. 我们考虑这样一件事情，如果两个输入的x足够接近那么实际上我们就不需要把他们两个都作为center，基于距离的运算让我们可以根据距离把数据划分成几个cluster从而作为prototype

   因此我们需要作两件事情：

   - 对于X进行分类
   - 为每一个类找到一个中心

   ![image-20200220185829902](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220185829902.png)

2. 下面我们来最优化这个问题。![image-20200220190251296](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220190251296.png)

这是一个两组变量的优化问题，突然想到多年以前中学时代的调整大法（固定一些，调整一些）这人里我们轮流调整。

- 固定中心，找到和中心最近的点归入那一类![image-20200220190302638](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220190302638.png)
- 固定分类，让center是这个类的平均值![image-20200220190316490](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220190316490.png)

3. 总结一下K-means算法的流程：

   搞清楚两件事情：有头有尾

   - 头：随机初始化centers（从x里面挑）
   - 尾：当每个类别里面的元素基本上不动了就停止（给足够的训练量）

   ![image-20200220190404905](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220190404905.png)

4. 把K-Means算法用到RBF网络里面：

   ![image-20200220190947360](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220190947360.png)

不过RBF Network。。。有点过时了，不过学习他相当于对于我们之前的其他内容也做了一个总结。