# Kernel Logistic Regression

这篇笔记开始介绍SVM的一些实际应用，本篇是对于logistic regression的改进

### 一、SVM as Regularization Model

Recap:

![image-20200206194709012](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206194709012.png)

对于松弛变量$\xi_n$的解释

![image-20200206194947552](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206194947552.png)

我们就得到了$\xi_n$的公式：
$$
\xi_n = \max(1-y_n(w^T)z_n+b,0)
$$

<font color = red>由此观之，SVM项可以看作是regularization项，这就得到了SVM Loss</font>

![image-20200206195531195](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206195531195.png)

我们总结一下各种regularization：

其中L2 regularization是对regularization constraint的一种改进：

![image-20200206195704389](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206195704389.png)

C越大，正则化的程度就越小！



### 二、SVM and Logistic Regression

对比0/1 loss 和 SVM loss：

![image-20200206200107468](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206200107468.png)

<font color = red>**注意一个结论：如果loss function 是0/1-loss的 upper bound，那么就可以优化该loss-function来进行操作** </font>

利用SVM-Loss的regression叫做：hinge regression

引入logistic regression里面的loss function

![image-20200206200200868](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206200200868.png)

对比三者，发现SVM-Loss 和 L2-regularized logistic regression类似！

![image-20200206200314403](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206200314403.png)

对比一下,$y_s\geq 1$的时候hinge和0/1一样



### 三、SVM 用作 binary classification

1. naive idea： 直接拿SVM-loss来代替logistic regression 或者 把SVM的结果作为gradient descent的初始解

![image-20200206200959690](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206200959690.png)

2. 融合两个模型进行一步操作：

![image-20200206201037591](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206201037591.png)

我们就有一个新的SVM模型了

3. 我们总结一下流程: Patt's Model

![image-20200206201307767](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206201307767.png)

*做的比较好的时候，一般A>0,B≈0*

4. 存在的问题：

   没有办法直接在 $z-space$里面找到自己想要的解!



### 四、Kernel Logistic Regression

1. Recap:

   我们可以使用kernel trick的一个原因是因为W可以表示成关于Z的一个线性组合，进而我们算出来的score就是关于Z的一个内积。

![image-20200206201829155](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206201829155.png)

Key: W是Z的线性组合！

**模型的系数是模型的本质差异**

2. Representer Theorem: 表示定理

![image-20200206202015223](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206202015223.png)

L2正则化的模型，我们最优的结果W一定都是关于Z的一个线性组合

2. KLR：

![image-20200206213248263](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206213248263.png)

我们利用结论，把关于$w$的最佳化问题转化成了关于$\beta$的最佳化问题。我们有如下的结论：

![image-20200206213457549](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206213457549.png)

