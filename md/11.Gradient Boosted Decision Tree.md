# Gradient Boosted Decision Tree

### 一、Adaptive Boosted Decision Tree

1. 对比 **Decision Tree** 和 **AdaBoost-Tree**

   我们总结过关于几类aggregation的例子

   - Bagging (boostrap) - uniform
   - AdaBoost - non-uniform
   - Decision Tree - conditionally  

   *uniformly的Decision Tree就是Random Forest*

   *配合weights的Decision Tree就是AdaBoost-DTree*

![image-20200218165858916](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218165858916.png)



2. 于是我们试图在Decision-Tree中加入权重，但是我们希望让DT成为一个黑盒子而不是直接对这个模型进行大幅度的改动，因此我们希望抽样的时候就按照权重的比例来抽样，从而我们不需要再对于原来的算法进行修改：

   ![image-20200218170218703](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218170218703.png)

   

3. 我们不希望有一棵树太强，这样整个模型的效果就是去了，于是我们希望通过剪枝来改善，或者仅仅是限制树的高度。

   ![image-20200218170640178](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218170640178.png)

   

4. 极端的剪枝条件下，这个AdaBoost-DT就是一个AdaBoost-Stump

   ![image-20200218170738189](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218170738189.png)



### 二、Optimization View of AdaBoost

1. 回忆一下在AdaBoost中权重的更新方法，我们有如下的推导

   ![image-20200218170829207](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218170829207.png)

AdaBoost当中的权重和 voting score有着一些关系



2. 类比我们在SVM中得到的Margin，我们可以看出实际上voting score是一种有符号且没有被归一化的margin，进而我们需要让voting score是很大的正数就需要$u_n^{T+1}$足够小

<font color = blue size = 4>结论：AdaBoost 可以减小$\sum^N_{n=1}u_n^{t}$</font>

![image-20200218174843390](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218174843390.png)

$\widehat{err}_{ADA}(s,y) = e^{-ys}$   exponential error measure，它作为$err_{0/1}$的一个upper bound可以作为error function的度量 

3. 对于$\widehat{err}_{ADA}(s,y) = e^{-ys}$我们也希望通过gradient descent的方法来减少这个误差。但是这个时候我们不知道对于权重的梯度到底是多少，我们先照搬一下：

![image-20200218175159530](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218175159530.png)

我们加入一个$\eta h(x_n)$相当于走了一小步，就像梯度下降中向梯度方向走的那一步，我们做代数变换，以及泰勒展开得到了是
$$
\widehat{E}_{ADA} \approx \sum_{n=1}^Nu_n^{(t)}-\eta\sum_{n=1}^Nu_n^{(t)}y_nh(x_n)
$$
于是问题转换为找到 **good h (direction of function)**
$$
s.t.minimize\ \sum_{n=1}^Nu_n^{(t)}(-y_nh(x_n))
$$
函数的方向就类似于梯度方向。接着我们做代数变形

![image-20200218175920189](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218175920189.png)

那么现在我们就需要让$E_{in}^{u^(t)}(h)$越来越小就行了，于是我们需要做的就是利用AdaBoost中的$A$降低它即可，我们通过演算法得到的$g_t=h$就是我们希望得到的结果，于是我们以此降低了$\widehat{E}_{ADA}$

4. 设置blending的权重：

![image-20200218180312494](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218180312494.png)

我们首先通过最小化$\widehat{E}_{ADA}=\sum_{n=1}^Nu_n^{(t)}e^{-y_n\eta h(x_n)}$来找到最好的$h=g_t$，也就是**function direction**紧接着我们希望利用这个学到的$g_t$来得到相应的$\eta$（事实上刚才我们并没有引入$\eta$这个参数，或者说$\eta=1$），接下来就是需要找到一个$\eta_t$，它每一次都找到下降最快的位置。这个方法被称为是 **steepest decent for optimization**也就是所谓的**“最速下降法”**

我们分析一下这个求和式，根据$y_n,g_t(x_n)$的关系可以分成两种情况，分别计算，其中:
$$
y_n = g_t(x_n):u_n^{(t)}e^{-\eta}	\ \ \ \ (correct)
$$

$$
y_n \neq g_t(x_n):u_n^{(t)}e^{+\eta}	\ \ \ \ (incorrect)
$$

$$
\widehat{E}_{ADA}=(\sum_{n=1}^Nu_n^{(t)})*((1-\epsilon_t)\ e^{-\eta}+\epsilon_t\ e^{+\eta})
$$

通过对$\eta$求导，我们得到了如下的式子，注意结果正好是我们之前使用的$\alpha_t$:
$$
\eta_t = ln\sqrt{\frac{1-\epsilon_t}{\epsilon_t}}=\alpha_t
$$
因此AdaBoost可以看作是近似函数梯度的最速下降法！



### 三、Gradient Boosting

我们把AdaBoost的思想进行进一步的推广，我们希望得到更加普适的结论，在AdaBoost中我们的h(x)是二元分类的假设函数，输出是二值的，现在我们不对h(x)作任何限制(可以是回归可以是分类)

![image-20200218201237767](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218201237767.png)

Regression: MSE

![image-20200218201310705](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218201310705.png)

我们首先让min_h项最小，通过泰勒展开配合对于voting score的求导我们得到了$h(x_n)=-\infin*2(s_n-y_n)$

在对于h没有任何限制的情况下我们可以得到这个解。显然还不太恰当。我们看看在有条件限制情况下它的表现

事实上h的量级不是十分重要，因为我们之后学习的是$\eta$,接下来我们为了避免naive solution我们采取正则化的操作，就是不希望h太大：

![image-20200218202447318](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218202447318.png)

 通过化简，我们看出实际上我们就是求对于余数 residual的MSE误差。简言之，对于regresion的GradientBoost就是对于residuals来说MSE的最小的$g_t=h$ 

搞定了$g_t$我们考虑怎么学习$\eta$，这个时候实际上就是一个对于 residual 的linear regression:

![image-20200218203523441](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218203523441.png)

因此，GB for regression就是找到最好的 $\alpha_t=\eta$，方式是通过$g_t$转化的linear regression。

关于它的解：

![image-20200218204733667](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218204733667.png)

下面对于GDBT做一个总结：

![image-20200218204016565](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218204016565.png)

1. 通过回归算法获$g_t$使得$MSE(X_n,y_n-s_n)$最小
2. 利用单变量线性回归得到 $\alpha_t = LR(g_t(x_n),y_n-s_n)$
3. 更新voting score $s_n = s_n + \alpha_tg_t(x_n)$
4. 重复若干次
5. 返回 $G(x) = \sum_{t=1}^T\alpha_tg_t(x)$

GBDT在回归中用的特别多，和AdaBoost-DTree是“回归兄弟”

### 四、Summary of Aggregation Models

下面对于各种模型进行一个总结：

首先是Aggregation-learning中的几个基本类型，实际上boosting类型的算法是最为管饭应用的

![image-20200218204755015](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218204755015.png)

其次是Aggregation of Aggregation的几个模型，在前人的基础上再创新高，三者应用都很广泛。

![image-20200218205007495](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200218205007495.png)