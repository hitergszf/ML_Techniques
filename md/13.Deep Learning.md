# Deep Learning

### 一、Deep Neural Network

1. 对于神经网络的物理解释回顾：神经网络的每一层实际上做的事情就是一种transformation，而transformation可以看作一种encoding也可以看作是一种特征的提取。

2. 对于浅层网络和深层网络的对比：![image-20200220152504795](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220152504795.png)

关于为什么deep learning更好用这个其实在吴恩达的deeplearning.ai中的解释大概如此：deep layers中层与层之间相当于是一种排列组合的方式让我们更好的能得到结果，而只是让某一层比较胖类似于加法原则，这样不能够很好的枚举所有的情况——或者说相同情况下需要的变数是更多的。

3. 深度学习的意义在于，可以在每一层提取一些简单的特征，一层一层下去把简单的特征组合在一起从而得到了比较复杂的特征。（CS231N的visualization）
4. 深度学习中的挑战和关键技巧：

- 困难的结构化设计：需要一些domian knoledge来设计architecture
- 模型复杂度高：通过大数据可以解决；正则化可以增强对于噪声的容忍程度：
  - dropout(优胜劣汰淘汰辣鸡神经元)
  - denoising(对于不太好的输入效果依然坚挺)
- 困难的优化问题：需要小心的initialization反正不太好的local minimum（一般到不了global minimum），这一步叫做pretraining
- 计算力需求大：提高硬件设施GPU TPU等

最显著的问题：

- regularization——结构化风险
- initialization——初始化

![image-20200220154127052](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220154127052.png)

5. 一般DL：

   ![image-20200220154158916](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220154158916.png)

### 二、Autoencoder

1. 权重可以看作编码：我们希望这种编码可以保留最多的信息。![image-20200220154237862](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220154237862.png)

2. 为了实现这个，我们可以先编码再解码：![image-20200220154332589](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220154332589.png)

我们利用非线性的tanh来学习到一种identical的机制，也就是学习到相应的权重。identity function的机制作用如下![image-20200220155006142](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220155006142.png)

监督领域下我们试图发现对于输入数据的一个有效的特征转换，往往转换有着降维的效果，同时保留着代表性的数据

无监督领域下我们试图进行一些潜在结构的学习，比如密度估计或者排除一些无关的特征。

所谓自动编码器就是通过近似全等映射学习数据的表示

3. 基本的Autoencoder: $d-\widetilde{d}-d \ NNET$

   一般情况下hidden layer是降维处理的，正则化的时候我们需要让译码器和编码器的权重一样，一般这时候计算量会增加一些：![image-20200220164836215](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220164836215.png)

   总结一些用自动编码器预训练的过程：![image-20200220165158305](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220165158305.png)

   

### 三、Denoising Autoencoder

1. 总结一下DL中 regularization的操作：

   ![image-20200220165500609](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220165500609.png)

- 一些限制条件（比如加入拉格朗日因子）
- weight decay 或者 weight elimination regulizers
- early stopping

2. 介绍一种新的方法：人为加入噪声使得模型鲁棒性更加强，通知去噪得到无噪的数据：![image-20200220170959366](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220170959366.png)

   我们增加噪声，把输出作为无噪的数据从而学习到一些去噪的自动编码器，实际上这个效果往往更好。噪声可以看作是一种人为的正则化操作了。

   

### 四、Principal Compunent Analysis

1. 考虑了非线性的去噪方法（例如tanh）我们回过头来看看线性的模型的效果：![image-20200220171411093](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220171411093.png)

向量化之后的表示十分简单（考虑到正则化的时候让译码解码的权重相等，我们就有）
$$
h(x) = WW^Tx
$$
对应的error function我们有：
$$
E_{in}(h)=E_{in}(W) = \frac1N\sum_{n=1}^N||x_n-WW^Tx_n||^2\ with\ d\times\tilde{d}\ matrix\ W
$$
由于eigen-decompose的定理我们直到，$WW^T$可以分解成$V\Gamma V^T$，$\Gamma$是一个对角阵，V是一个正交阵![image-20200220172107854](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220172107854.png)

我们对于$x_n$也作类似的分解。我们试图找到最优化的$\Gamma$和V：

首先是$\Gamma$：因为试图最小化，所以我们去掉左边的V最小化得到的参数是一样的，因为V的效果在几何上就是一个旋转的操作![image-20200220172307131](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220172307131.png)

我们直到$rank(\Gamma)\leq\tilde{d}$所以最好的情况下我们作差的结果是$I_{\tilde{d}}$在左上的形式，其余部分都是0。这种情况下我们的差的模长是最小的。因此最好情况下的$\Gamma$可以表示为：
$$
\Gamma = 
\left[\begin{matrix}I_{\tilde{d}} & 0 \\0 & 0 
\end{matrix}\right]
$$
接下来我们考虑第二步优化：![image-20200220173056120](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220173056120.png)

我们转换一下视野，改成最大化。考虑$\tilde{d}=1$的简单情形：
$$
\max_v{\sum_{n=1}^Nv^Tx_nx_n^Tv} \ \ subject\ \ to\ v^Tv=1
$$
我们利用拉格朗日因子增加相应的项并且求导一下：
$$
\sum_{n=1}^N(x_nx_n^Tv-\lambda v)=0
$$
我们转换角度看这个式子，其实就是说明$\lambda$是特征值！v是特征向量

那么我们希望这个式子最大就是希望我们v包含着矩阵$X^TX$中所有最大的特征值。

因此我们推广到一般的$\tilde{d}$就有了结论。

​	**线性自动编码器就是寻找到一个最匹配$x_n$的正交模式$w_j$来投影**



2. 我们看一下更为常用的PCA方法：

   多了第一步取平均的动作，最后回传的也是中心为0的特征转换

   ![image-20200220174415262](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200220174415262.png)

它在统计上有着很好的降维的作用，常用于数据的预处理。