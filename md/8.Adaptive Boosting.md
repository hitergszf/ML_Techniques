# Adaptive Boosting

### 一、Motivation of Boosting

我们启发式的算法基于小学老师教同学辨认苹果的这一个事实：

1. Tony: Red!

   <font color = red>Teacher: Almost Right! Let's look at the rest! (Then enlarge the *MISTAKES*!)</font>

2. Jack: Circular!

   <font color = blue>Teacher: Almost Right! Let's look at the rest! (Then enlarge the *MISTAKES*!) And Then merge the 2 principles!</font>

3. Jessica: Stem!

   <font color = purple>Teacher: Almost Right! Let's look at the rest! (Then enlarge the *MISTAKES*!) And Then merge the 3 principles!</font>

最后得出的其实是综合每个同学的结果的出来的模型选择，其中老师的作用是让这些错误的类别被放大以至于学生可以学习的更好。这一点在perceptron里面也可以体现出来，总结如下。

![image-20200208212329502](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208212329502.png)

接下来我们希望数学证明这个想法的可行性。

### 二、Diversity by re-weighting

1. 之气在bagging中我们是采用uniformly的方式也就是说每个$g_t$预测的权重都是1，现在我们希望对于这些权重进行一些改进，就好比老师对于那些错误的放大一般。我们对于错误进行一个权重衡量，看bootstrapping之后所有的点出现的次数作为实际衡量中的err权重

   ![image-20200208212636891](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208212636891.png)

2. 接下来考虑的是如何 re-weighting这些权重呢，因为我们一开始对于数据集boostrap应该和re-weighting一起进行所以我们应该想一下其他的方法。

   ![image-20200208213030093](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208213030093.png)

我们迭代的想如果$g_t$对于$u^{(t+1)}$不太好我们就应该应该返回一个更好的$g_{t+1}$ 

3. 我们回忆一下啊抛硬币的过程，这个时候下面的式子应该是1/2，也就是说我们希望随机性是最大的，这样我们的diversity才能最大。那么我们试图打到下面式子的条件！![image-20200208213225835](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208213225835.png)

4. 过程如下：

   ![image-20200208213359840](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208213359840.png)

简言之就是每一次re-scaling，通过乘以对立面的系数来达到这一点。于是我们实现了diversity。



### 三、 Adaptive Boosting

1. 接下来我们讲一下这个算法的实现过程：利用Scaling Factor来实现re-weighting

![image-20200208213612727](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208213612727.png)

2. 一个算法的初步想法大概如下：

   ![image-20200208213748810](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208213748810.png)

我们考虑一下$u^{(1)}$的初始化，就希望最简单——$\frac1N$ ，至于G(x)的方法其实都可以用，我们的adaptive boosting用的是non-uniformly的$\alpha_t$。

3. AdaBoosting的算法实现：

   ![image-20200208213959865](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208213959865.png)

总结一下AdaBoosting做的事情：

- 学生：基本的算法 base learning algorithm

- 老师：Scaling Factor放大错误来更好的学习

- 班级: 用on the fly 的方式顺便学习了 $\alpha_t$:
  $$
  \alpha_t = \ln(\sqrt{\frac{1-\epsilon_t}{\epsilon_t}})
  $$
  最后学习到的<font color = red> $G(x) = sign(\sum_{t=1}^T \alpha_tg_t(x))$ </font>

![image-20200208214715375](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208214715375.png)

当然A的选择也至关重要，可以有的选择很多。



### 四、Theoretical Guarantee

最后是根据VC bound的出来的一些理论上的保证

![image-20200208214826040](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208214826040.png)