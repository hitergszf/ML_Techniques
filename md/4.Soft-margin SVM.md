# Soft Margin SVM

### 一、Primal Soft Margin SVM

我们使用Gaussian kernel的时候over fitting的原因可能是因为参数选的不好，也有可能是我们的限制条件太苛刻——我们可以适当放宽条件，允许一些错误，于是这就从hard-margin转化到了soft-margin，借助pocket算法找到灵感，也就是找到犯错最少的而不是不犯错的模型

![image-20200206185030223](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206185030223.png)

我们引入C来表示**large margin**和**noise tolerance**之间的相对重要性。

但是存在两个问题：

1. 不是linear的问题，也就是不是QP问题我们就没有办法解决了。
2. 无法分辨错误的程度，也就是对于错误的类型没有一个定量的认知。

我们引入新的模型，使用变量$\xi_n$记录错误程度：

![image-20200206185501847](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206185501847.png)

​	C用来在large margin和margin violation之间作权衡：

![image-20200206190109411](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206190109411.png)



### 二、Dual SVM

引入lagrange因子来转化为Dual Problem:

![image-20200206190305923](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206190305923.png)

利用KKT condition简化问题！

先对$\xi_n$进行求导，我们有如下：

![image-20200206190408284](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206190408284.png)
$$
C = \alpha_n+\beta_n\
$$
由于我们有$\beta_n\geq0$所以就得到了
$$
0\leq\alpha_n\leq C
$$
然后我们也把$\xi$干掉了

再对$b,w$进行求导，其实和之前的推导类似：

![image-20200206190944026](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206190944026.png)

最终问题划归为如下问题：

![image-20200206191022792](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206191022792.png)

N variables, 2N+1 constraints!

### 三、求解Dual Soft-SVM

 ![image-20200206192005685](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206192005685.png)

这里我们来单独说一下b的求解，和hard margin是否一样呢，之前hard margin使用complimentary slackness

![image-20200206192113014](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206192113014.png)

找一个 free SV（一般情况下都会有free SV)

看一下Soft-Margin SVM,也有可能over fitting

![image-20200206192807525](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206192807525.png)

试图解释一下这个模型：

![image-20200206192843033](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206192843033.png)

- free SV: 正好在边界上
- non SV: 在边界外
- bounded SV：在边界内（违反但分类正确，违反且分类错误）<font color=red>唯一有可能犯错的点！</font>

所以，$\alpha_n$可以用来分析这个模型的数据



### 四、模型的选择

使用validation方法来选择

对于SVM的$E_{loocv}$，我们有一些特别的结论：

![image-20200206193919658](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206193919658.png)

non-SV对于最佳解g没有什么影响！

所以SV数量可以作为safety-check！

