# Blending and Bagging

### 一、简介

我们像调制鸡尾酒一样来对待我们备选的模型，我们大致有几种方案：

1. 最佳原则——validation
2. uniformly vote
3. non-uniformly vote
4. conditionally

建模，假设有T个备选的g，$g_1,g_2,...,g_T$
$$
G(x) = g_{t_*}(x)\\
t_* = argmin_{t\in{1,2,..,T}}E_{val^-}(g_t^-)
$$

$$
G(x) = sign(\sum^{T}_{t=1}1*g_t(x))
$$

$$
G(x) = sign(\sum^{T}_{t=1}\alpha_t*g_t(x))\\
with \ \alpha_t\ge 0
$$

$$
G(x) = sign(\sum_{t=1}^Tq_t(x)g_t(x))\\
with \ q_t(x)\ge0
$$

可以发现最后一种情况包山包海，可以把前三种情况算在它的范畴里面



### 二、Uniform Blending

1. Classification:

$$
G(x) = sign(\sum^{T}_{t=1}1*g_t(x))
$$

其实类似于uniform voting for multiclass
$$
G(x) = argmax_{1\le k\le K}\sum_{t=1}^T[[g_t(x)=k]]
$$

2. Regression:

$$
G(x) = \frac1T\sum_{t=1}^Tg_t(x)
$$

<font color = red>3. Theoretical Analysis of Uniform Blending: </font>

![image-20200208162512894](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208162512894.png)

<font color = blue>4. 传说中的Bias Variance 分析:![image-20200208162618819](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208162618819.png)</font>





### 三、Linear and Any Blending

1. Linear Blending 和 LinReg+transformation的相同之处：

我们先知道$g_t$然后利用这个来计算$\alpha_t$因此实际上类似于一个2-level的学习过程，类似于probabilistic的SVM模型，但是这里有一个区别就是他的$\alpha_t$是＞0的。

![image-20200208162741399](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208162741399.png)

![image-20200208162922357](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208162922357.png)

在实际过程中我们常常忽略constraints，把非正的结果想做事投反对票的思维。

2. 对比一下linear blending 和selection的模型，利用VC-dim来进行一些分析，我们发现其实这样做十分容易过拟合

   ![image-20200208163411651](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208163411651.png)

我们在selection的时候要付出的VC-dim的代价已经很大了现在blending就更危险了。（因为selection其实是一种特例）

3. 其他的blending，又叫做stacking：

![image-20200208163631581](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208163631581.png)

危险而诱人！

### 四、 Bagging (Bootstrap Aggregation)

1. 我们之前讲的方法都是blending，也就是混合鸡尾酒，但是事实上在learning的过程中我们希望找每个$g_t$的过程不需要在aggregate之前，这样的话我们可以提高效率，换句话说我们就是希望能让找$g_t$和blending能同步进行。
2. 还有一件特别重要的事情就是，我们尽量让$g_t$尽可能不同，发挥每个模型的特点的作用就是要让他们diverse这些diversity总结如下：

![image-20200208205042283](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208205042283.png)

保持diversity的方式其实就是让data尽量保持随机性。

3. 保持随机性的方式比较好的就是使用同分布的很多数据，但是很多情况下数据是有限的，我们希望在有限的数据中发挥数据的最大效果，这就需要我们对于数据集做一点操作。于是我们就引入了bootstrapping aggregation（也就是bagging）

   ![image-20200208205817092](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208205817092.png)

4. 所谓bootstrapping的方法其实很简单，其实就是对于N个数据量的data做一次放回的抽样。

我们对比一下理想的学习过程和bootstrap的过程（实际情况下我们只能选择bagging而不是真的要再找规模为N的数据量）

![image-20200208205958729](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200208205958729.png)

bagging相当于是在base algorithm A之上的meta algorithm，其实是对取样的操作。

我们bootstrap最后对于G的选择是uniform形式的。

对接下来的模型做一个总结：

<font color = red size = 4>1. Bagging —— uniformly vote </font>

<font color = orange size = 4>2. AdaBoost ——  non-uniformly vote</font>

<font color = green size =4>3. Decision Tree & Random Forest —— conditionally</font>