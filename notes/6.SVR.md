# Support Vector Regression



上一篇介绍对于logistic regression的kernel方法，这次介绍对于一般的回归使用kernel的方法

### 一、Kernel Ridge Regression

我们采用L2-正则化的时候需要解决这样一件事情。配合表示定理我们转化为关于$\beta$的问题，这里直接用的是square-error。

![image-20200206214830497](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206214830497.png)

![image-20200206214840995](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206214840995.png)

<font color =red>**我们使用kernel matrix的test时候第一个矩阵是X_train,第二个是X_test，因为最终得到的g是用X_train里面的模型进行线性组合得到的，所以训练的时候需要用X_train,X_train，测试的时候用的是X_train,X_test**</font>

对比一下两个模型：

![image-20200206214855977](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206214855977.png)

### 二、Support Vector Regression Primal

这里引入tube regression对于SVM standard进行微调

1. 对比一下Soft-Margin和Least Square两种情况的模型：![image-20200206220232697](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206220232697.png)

这里我们的$\beta$比较dense，我们希望有和standard SVM一致的sparse 特性，利用SV改进loss function

2. 考虑一下Tube Regression:

我们把cost function重新定义

![image-20200206220313186](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206220313186.png)



![image-20200206220344539](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206220344539.png)

![image-20200206220433809](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206220433809.png)

我们模仿标准SVM的模型得到现在的模型

<font color = red>注意我们对于regularization的最小化是对于loss function而对于SVM本质上是对于margin的最小化，所以两者有差别！！！</font>

![image-20200206221502808](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206221502808.png)

这就是SVR primal，我们使用新的loss function而不是square或者一般的svm

![image-20200206221636323](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206221636323.png)

$\epsilon$是可选的，这个是比standard多的变量

### 三、Support Vector Regression Dual

我们用使用Dual模型对它进行改进，同之前我们引入拉格朗日因子

![image-20200206222229537](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206222229537.png)

利用KKT条件我们推导一下。

我们看一下Dual相似性：

![image-20200206221939223](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206221939223.png)

回到之前关于sparsity的解决

![image-20200206222019216](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206222019216.png)

SVR提供了sparse $\beta$



### 四、对于核方法的总结

1. 总结一下

首先是linear部分：

![image-20200206223224252](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206223224252.png)

然后我们有kernel形式的:

![image-20200206223312455](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20200206223312455.png)

probabilistic SVM是2-level的方法，先SVM再logistic regression

2. 选择：

- 第一排不怎么用

- 第三排不怎么用，dense data! 

  

